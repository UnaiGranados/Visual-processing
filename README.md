# 3D vision system for tag detection and dynamic control of robotic arm in ROS

This ROS metapackage contains specific robot setup configuration, tag detections  and robot control files.

## 1. Dependencies

Tecnalia repositories:

* `algebra_libraries`
* `manipulation`


Additional dependencies:

* `fanuc`
* `fanuc_gazebo`
* `industrial_robot_simulator`
* `robot_state_publisher`
* `rviz`
* `rviz_visual_tools`
* `realsense-ros`
* `realsense_gazebo_plugin`
* `moveit_calibration`
* `moveit_visual_tools`

## 2. Installation

### 2.1. Create catkin workspace

```shell
$ mkdir -p <ws_folder>/src
$ cd <ws_folder>
```

### 2.2. Initialize workspace

```shell
<ws_folder> $ catkin init
```

### 2.3. Download source code

```shell
<ws_folder> $ cd src
<ws_folder>/src $ git clone git@git.code.tecnalia.com:tecnalia_robotics/internships/unai-granados/fanuc_3d_cam.git
```

### 2.4. Synchronize depencies

```shell
<ws_folder>/src $ wstool init
<ws_folder>/src $ wstool merge <package_folder>/.rosinstall
```

### 2.5. Clone repositories defined in the `.rosinstall` file                                                                                                                                                                                                                              
```shell
<ws_folder>/src $ wstool update -t .
```

### 2.6. Install dependencies

Before running `rosdep install` make sure lists are updated with `sudo apt update` and `rosdep update`.

```shell
<ws_folder>/src $ rosdep install --from-paths . -i --rosdistro=<ros-distro> -y
```

### 2.7. Compile workspace

```shell
<ws_folder>/src $ catkin build
```

## 3. Structure

This is a set of packages to perform  visual servoing functionalities. The current implementation includes the following three main packages:

* `flexbotics_cr7ial_support`: configuration, scene URDF and launch files for the FANUC CR-7iA/L robot.
* `flexbotics_cr7ial_moveit_config`: *MoveIt! Setup Assistant* generated package.
* `visual_processing`: this is where the nodes and other packages that perform the actual work are implemented.


## 4. Usage

### 4.1. Robot setup test scenario

The scene with the FANUC CR-7iA/L robot can be launched in as follows:

```shell
$ roslaunch flexbotics_cr7ial_support set_up_cr7ial_scene.launch
```

By default, the scene will be launched with a simulated robot making use of the `industrial_robot_simulator` package. To launch the scene with the real robot set the `use_sim` argument to `false`:

```shell
$ roslaunch flexbotics_cr7ial_support set_up_cr7ial_scene.launch use_sim:=false
```

By default simulation will be visualized on Gazebo with a simulated RealSense D435 camera. To launch scene with the real camera set `use_gazebo` and `use_rs_gazebo` arguments to `false`:

```shell
$ roslaunch flexbotics_cr7ial_support set_up_cr7ial_scene.launch use_gazebo:=false use_rs_gazebo:=false
```

For instructions about how to use the real robot see the [tecnalia_robotics/fanuc/Documentation](https://git.code.tecnalia.com/tecnalia_robotics/fanuc/documentation) repository.

### 4.2. Camera calibration

There is a package that has been implemented to calibrate the RealSense D435 depth camera, see the [tecnalia_robotics/flexbotics_calibration_suite](https://git.code.tecnalia.com/tecnalia_robotics/flexbotics_calibration_suite) repository.

First, launch the following launchfile:

```shell
$ roslaunch flexbotics_calibration_suite flexbotics_calibration_suite.launch
```
With this launch, a GUI will open.Then choose the type of calibration needed and start the the calibration procedure. [manipulator_commander] is needed for the calibration.

### 4.3. Tag detection

The approach implemented here is based on different python nodes to detect a marker/tag and compute the transform between camera and tag using OpenCV. To get this result, has been implemented two methods:

The firts one is using RGB image and tag model dimension to calculate the rotation and translation of the tag respect to the camera:

```shell
$ python tf_transform_rgb.py
```
The other one is using both RGB and depth image:

```shell
$ python tf_depth.py
```

### 4.4. Move robot 

The idea behind the calibration validation is to scan an element with a known geometry and check the accuracy of the reconstructed pointcloud from the laser readings. The program that allows capturing the data is implemented in the `laser_calibration/src/calib_validation_node.cpp` node. Data is generated by performing a linear movement with the robot and storing robot poses and laser readings in separate CSV files.

When operating with the real robot, the robot has to be positioned with the teach pendant over the element that we want to scan and the program will perform a linear movement of 10 cm along the X-axis of the base of the robot while the laser is active. The program can be launched with the following command:

```shell
$ roslaunch laser_calibration calib_validation.launch simulate_devices:=false output_folder:=<folder>
```

This launchfile has to be executed on its own without having to launch the previously described launchfiles for robot or laser configuration. Note that the output folder where the two CSV files will be stored has to be specified. In this example the `simulated_devices:=false` argument is also passed to make use of the real robot and laser instead of running simulated devices.
